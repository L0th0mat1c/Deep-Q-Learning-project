{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from random import randint\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create env\n",
    "\n",
    "env=gym.make(\"Taxi-v3\", render_mode=\"ansi\") #\"human\", \"ansi\", \"rgb_array\"\n",
    "env.reset()\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action size 6\n",
      "State size 500\n"
     ]
    }
   ],
   "source": [
    "# create Q table\n",
    "\n",
    "action_size=env.action_space.n\n",
    "print(\"Action size\", action_size)\n",
    "\n",
    "state_size=env.observation_space.n\n",
    "print(\"State size\", state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "qtable=np.zeros ((state_size, action_size))\n",
    "print(qtable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Episodes\n",
    "\n",
    "total_episodes = 100000\n",
    "total_test_episodes = 1000\n",
    "max_steps = 99\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.7     #learning rate (step size on map)\n",
    "gamma = 0.618           #Discounting rate\n",
    "\n",
    "#exploration parameters\n",
    "\n",
    "epsilon = 1.0           #Exploration rate\n",
    "max_epsilon = 1.0       #Exploration probability at start\n",
    "min_epsilon = 0.01      #Minimum exploration probability\n",
    "decay_rate = 0.01       #Exponential decay rate for exploration prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 99900\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "for episode in range(total_episodes):\n",
    "    # Reset env for start\n",
    "    state = env.reset()[0]\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0  \n",
    "    for step in range(max_steps):\n",
    "            # 3. Choose an action a in the current world state (s)\n",
    "            ## First we randomize a number\n",
    "            exp_exp_tradeoff = random.uniform(0,1)\n",
    "            \n",
    "            ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "            if exp_exp_tradeoff > epsilon:\n",
    "                action = np.argmax(qtable[state,:])\n",
    "            \n",
    "            # Else doing a random choice --> exploration\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "            \n",
    "            # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "            new_state, reward, done, info, _ = env.step(action)\n",
    "            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "            qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state,:]) - qtable[state, action])\n",
    "            \n",
    "            # Our new state is state\n",
    "            state = new_state\n",
    "            \n",
    "            # If done : finish episode\n",
    "            if done == True: \n",
    "                    break\n",
    "        \n",
    "            # Reduce epsilon (because we need less and less exploration)\n",
    "            epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
    "    if episode % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {episode}\")\n",
    "\n",
    "print(\"Finished\")           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "          0.        ,   0.        ],\n",
       "       [ -2.50421541,  -2.43400544,  -2.50421536,  -2.43400544,\n",
       "         -2.32039715, -11.43400312],\n",
       "       [ -1.83910191,  -1.35777508,  -1.83910436,  -1.35778444,\n",
       "         -0.57891593, -10.35777005],\n",
       "       ...,\n",
       "       [ -2.1430876 ,   0.68134715,  -2.12586921,  -2.2253835 ,\n",
       "         -7.        , -10.58404736],\n",
       "       [ -2.36863532,  -2.32211961,  -2.36351949,  -2.13656497,\n",
       "        -10.75480486, -10.95144718],\n",
       "       [ -1.21282   ,  -0.91      ,  -1.21282   ,  11.36      ,\n",
       "         -7.        ,  -7.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 900\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "env.reset()[0]\n",
    "rewards = []\n",
    "\n",
    "for episode in range(total_test_episodes):\n",
    "    state = env.reset()[0]\n",
    "    step = 0\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "    \n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        env.render()\n",
    "        # Take the action (index) that have the maximum expected future reward given that state\n",
    "        action = np.argmax(qtable[state,:])\n",
    "        \n",
    "        new_state, reward, done, info, _ = env.step(action)\n",
    "        \n",
    "        total_rewards += reward\n",
    "        \n",
    "        if done:\n",
    "            rewards.append(total_rewards)\n",
    "            break\n",
    "        state = new_state\n",
    "    if episode % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {episode}\")\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_frames(frames, sleep_time, episodes, random):\n",
    "    if random:\n",
    "        random_number_list = []\n",
    "        for e in range(episodes):\n",
    "            random_number = randint(1, episodes)\n",
    "            random_number_list.append(random_number)\n",
    "        \n",
    "    for i, frame in enumerate(frames, start=1):\n",
    "        if random and frame[\"episode\"] in random_number_list:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            print(f\"Episode: {frame['episode']}\")\n",
    "            \n",
    "            print(frame['frame'])\n",
    "            \n",
    "            print(f\"Timestep: {i}\")\n",
    "            print(f\"State: {frame['state']}\")\n",
    "            print(f\"Action: {frame['action']}\")\n",
    "            print(f\"Reward: {frame['reward']}\")\n",
    "            \n",
    "            sleep(sleep_time)\n",
    "        elif not random and frame[\"episode\"] < episodes: \n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            print(f\"Episode: {frame['episode']}\")\n",
    "            \n",
    "            print(frame['frame'])\n",
    "            \n",
    "            print(f\"Timestep: {i}\")\n",
    "            print(f\"State: {frame['state']}\")\n",
    "            print(f\"Action: {frame['action']}\")\n",
    "            print(f\"Reward: {frame['reward']}\")\n",
    "            \n",
    "            sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "total_penalties = 0\n",
    "total_epochs = 0\n",
    "\n",
    "for i in range(episode):\n",
    "    # Reset the environment\n",
    "    state = env.reset()[0]\n",
    "    \n",
    "    # Initialize fields\n",
    "    epochs = penalties = reward = 0\n",
    "    done = False\n",
    "    \n",
    "    # Start the episode process\n",
    "    while not done:\n",
    "        # Only Exploitation during the evaluation phase\n",
    "        action = np.argmax(qtable[state])\n",
    "        \n",
    "        # Performing action inside the environment\n",
    "        state, reward, done, info, _ = env.step(action)\n",
    "        \n",
    "        # Put each rendered frame into a dict for animation\n",
    "        frames.append({\n",
    "            \"episode\": i,\n",
    "            \"frame\": env.render(),\n",
    "            \"state\": state,\n",
    "            \"action\": action,\n",
    "            \"reward\": reward\n",
    "        })\n",
    "        \n",
    "        # Getting stats when the agent performed illegal action (pickup or dropoff)\n",
    "        penalties += 1 if reward == -10 else 0\n",
    "        \n",
    "        epochs += 1\n",
    "    \n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results (999 episodes)\n",
      "Average timesteps: 13.037037037037036\n",
      "Average penalties: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Results ({episode} episodes)\")\n",
    "print(f\"Average timesteps: {total_epochs / episode}\")\n",
    "print(f\"Average penalties: {total_penalties / episode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2\n",
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | :\u001b[42m_\u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "\n",
      "Timestep: 37\n",
      "State: 177\n",
      "Action: 1\n",
      "Reward: -1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m print_frames(frames, sleep_time\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, episodes\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, random\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[30], line 21\u001b[0m, in \u001b[0;36mprint_frames\u001b[0;34m(frames, sleep_time, episodes, random)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAction: \u001b[39m\u001b[39m{\u001b[39;00mframe[\u001b[39m'\u001b[39m\u001b[39maction\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReward: \u001b[39m\u001b[39m{\u001b[39;00mframe[\u001b[39m'\u001b[39m\u001b[39mreward\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m     sleep(sleep_time)\n\u001b[1;32m     22\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m random \u001b[39mand\u001b[39;00m frame[\u001b[39m\"\u001b[39m\u001b[39mepisode\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m<\u001b[39m episodes: \n\u001b[1;32m     23\u001b[0m     clear_output(wait\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_frames(frames, sleep_time=0.1, episodes=1000, random=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
